from flask import Flask, request, jsonify
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.schema import SystemMessage, HumanMessage
from dotenv import load_dotenv
import os
import requests

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
QDRANT_URL = "http://localhost:6333"
COLLECTION_NAME = "conventions"

embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")
llm = ChatOpenAI(model="gpt-4o-mini")

app = Flask(__name__)

def fetch_convention_rules(vector):
    response = requests.post(
        f"{QDRANT_URL}/collections/{COLLECTION_NAME}/points/search",
        json={
            "vector": vector,
            "top": 5,
            "with_payload": True
        }
    )
    if not response.ok:
        return None, response.text

    rules = []
    result = response.json().get("result", [])
    for match in result:
        rule = match["payload"].get("rule")
        if rule:
            rules.append(rule)
    return rules, None


@app.route("/embed", methods=["POST"])
def embed():
    text = request.json.get("text")
    if not text:
        return jsonify({"error": "ë¦¬ë·°í•  ì½”ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."}), 400

    # ë²¡í„° ìƒì„±
    vector = embedding_model.embed_query(text)

    # ì»¨ë²¤ì…˜ ê²€ìƒ‰
    rules, error = fetch_convention_rules(vector)
    if error:
        return jsonify({"error": "Qdrant ê²€ìƒ‰ ì‹¤íŒ¨", "detail": error}), 500

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
    {rules}ëŠ” ì»¨ë²¤ì…˜ì…ë‹ˆë‹¤. 
    {text} 
    ì»¨ë²¤ì…˜ì— ë§ê²Œ ìœ„ ì½”ë“œë¥¼ ìµœëŒ€í•œ ìì„¸í•˜ê²Œ ë¦¬ë·°í•˜ì„¸ìš”.
    ë¬¸ì œì ì´ ì—†ëŠ” ë¶€ë¶„ì€ ë¦¬ë·°í•˜ì§€ ë§ˆì„¸ìš”.
    ë‹¤ìŒì€ í”¼ë“œë°± ìˆœì„œ ë° ì¹´í…Œê³ ë¦¬ì…ë‹ˆë‹¤.
    - **Observation**: ì½”ë“œì—ì„œ ê´€ì°°ëœ ì‚¬ì‹¤
    - **Interpretation**: í•´ì„ ë° ìœ„í—˜ ê°€ëŠ¥ì„±
    - **Suggestion**: ê°œì„  ë°©ì•ˆ ì œì•ˆ
    í•´ë‹¹ ë‚´ìš©ì— ë§ê²Œ ë¦¬ë·°í•˜ì„¸ìš”.
    """

    # ë©”ì‹œì§€ êµ¬ì„±
    messages = [
        SystemMessage(content="ë‹¹ì‹ ì€ ì „ë¬¸ ì½”ë“œ ë¦¬ë·°ì–´ AIì…ë‹ˆë‹¤. "),
        HumanMessage(content=prompt)
    ]

    # LLM ì‘ë‹µ
    response = llm.invoke(messages)
    print(rules)
    print(response.content)
    prompt02 = f"""
    ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ ë¦¬ë·° ë‚´ìš©ì„ ì•„ë˜ì— ì œê³µí•©ë‹ˆë‹¤. ì´ ë¦¬ë·°ëŠ” ì½”ë“œì— ëŒ€í•œ ì´ˆê¸° ë¶„ì„ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, í•­ëª©ë³„ë¡œ Observation, Interpretation, Suggestionì´ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ì¤‘ìš”ë„ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ìµœëŒ€ 5ê°œê¹Œì§€ ì •ë¦¬í•˜ì—¬ ì£¼ì„¸ìš”. ë¦¬ë·°ëŠ” ì¡´ëŒ“ë§ë¡œ í•˜ì„¸ìš”.
    
    **[L0-ë¦¬ë·°ë¶ˆê°€]**

    - PR Description, í…Œí¬ ìŠ¤í™ ë“± ì–´ë–¤ ì‘ì—…ì„ í–ˆëŠ”ì§€ ì¶©ë¶„í•œ ì„¤ëª…ì´ ì—†ëŠ” ê²½ìš°
    - ë³€ê²½ ì‘ì—…ì´ ë„ˆë¬´ ì»¤ì„œ ë¦¬ë·°ê°€ ì–´ë ¤ìš´ ê²½ìš°
    - ì„¤ê³„ê°€ ì˜ëª»ë˜ì–´ ì „ì²´ì ìœ¼ë¡œ ì¬ì‘ì—…ì´ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨í•´ ë¦¬ë·°ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ëŠ” ê²½ìš°
    
    **[L1-ë³€ê²½ìš”ì²­]**
    
    - ê¸°ëŠ¥ ê²°í•¨, ì‹¬ê°í•œ í€„ë¦¬í‹° ì €í•˜, íŒ€ ì»¨ë²¤ì…˜ ìœ„ë°˜ ë“± ë°˜ë“œì‹œ ë¨¸ì§€ ì „ì— ë³€ê²½ì´ í•„ìš”í•œ ê²½ìš°
    
    **[L2-ë³€ê²½í˜‘ì˜]**
    
    - ê°€ê¸‰ì  ë¨¸ì§€ ì „ì— ë³€ê²½ë˜ì—ˆìœ¼ë©´ ì¢‹ê² ì§€ë§Œ, ë°°í¬ í›„ í›„ì†ì‘ì—…ìœ¼ë¡œ ë°”ë¡œ ì§„í–‰ì´ ëœë‹¤ë©´ ê´œì°®ë‹¤ê³  ìƒê°í•˜ëŠ” ê²½ìš°
    - ì‘ì„±ìê°€ ì˜ê²¬ì— ë™ì˜í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ í† ë¡  í•„ìš”
    
    **[L3-ì¤‘ìš”ì§ˆë¬¸]**
    
    - ê¶ê¸ˆì¦ì´ í•´ì†Œë˜ì–´ì•¼ ì •í™•í•œ ë¦¬ë·°/í”¼ë“œë°±ì´ ê°€ëŠ¥í•˜ë‹¤ê³  íŒë‹¨í•˜ëŠ” ê²½ìš°
    
    **[L4-ë³€ê²½ì œì•ˆ]**
    
    - ì œì‹œí•œ ë°©ì•ˆì´ ë” ì¢‹ë‹¤ê³  ìƒê°í•˜ì§€ë§Œ ì‘ì„±ì íŒë‹¨ì— ë§¡ê²¨ë„ ë¬´ê´€í•œ ê²½ìš°
    
    **[L5-ì°¸ê³ ì˜ê²¬]**
    
    - ë” ì¢‹ì€ì§€ëŠ” ëª¨ë¥´ê² ìœ¼ë‚˜ ë‹¤ë¥¸ ë°©ë²• ì œì‹œ í˜¹ì€ ì°¸ê³ í• ë§Œí•œ ë‚´ìš©ì¸ ê²½ìš°

    ---
    ## ğŸ” Review Format

    ### Feedback (1~n)

    ### [ë²ˆí˜¸]. ìš”ì•½ ì œëª©
    - **Observation**: ì½”ë“œì—ì„œ ê´€ì°°ëœ ì‚¬ì‹¤
    - **Interpretation**: í•´ì„ ë° ìœ„í—˜ ê°€ëŠ¥ì„±
    - **Suggestion**: ê°œì„  ë°©ì•ˆ ì œì•ˆ
    - **Review Level**: [Lx: ë“±ê¸‰ëª…]

    ---
    ## ğŸ“‹ Final Checklist

    - [ ] L1~L2 ë“±ê¸‰ì— í•´ë‹¹í•˜ëŠ” ìˆ˜ì • ìš”ì²­ í•­ëª©
    - ( ) L3~L5 ë“±ê¸‰ì— í•´ë‹¹í•˜ëŠ” ì„ íƒì  ì œì•ˆ í•­ëª©

    ë‹¤ìŒì€ ì›ë³¸ ë¦¬ë·°ì…ë‹ˆë‹¤:

    {response.content}
    """

    response02 = llm.invoke([
        SystemMessage(content="ë‹¹ì‹ ì€ ë¦¬ë·° ê²°ê³¼ë¥¼ ìš”ì•½í•˜ê³  ì •ë¦¬í•˜ëŠ” í…œí”Œë¦¿ ë§ˆìŠ¤í„°ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì—­í• ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: 1. ë¦¬ë·° í•­ëª©ì„ ê°ê° ì‹ë³„í•˜ì—¬, 2. í˜•ì‹ì— ë§ê²Œ ì •ëˆí•˜ê³ , 3. ê° í•­ëª©ì— ëŒ€í•´ L0~L5 ë“±ê¸‰ ì¤‘ í•˜ë‚˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."),
        HumanMessage(content=prompt02)
    ])

    return response02.content

if __name__ == "__main__":
    app.run(port=5001)
